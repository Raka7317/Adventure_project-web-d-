{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNGsK5ENdpovEzBx1T3Fi7B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raka7317/Adventure_project-web-d-/blob/main/Copy_of_Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEEP LEARNING LAB\n",
        "DATE: 18-12-25\n"
      ],
      "metadata": {
        "id": "PXR7M3mmk6ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load MNIST (70,000 images, 28x28 = 784 features)\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "# Scale data (important for MLP)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# MLP model\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    batch_size=128,\n",
        "    max_iter=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"MNIST MLP Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHmd026tVcfB",
        "outputId": "6c46774a-0d89-4692-fe87-44df0a665774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST MLP Accuracy: 0.9712142857142857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Fa-eQuZtk4Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mEZco70JmZKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import Required Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# 2. Load & Preprocess MNIST Dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Normalize pixel values (0-255 -> 0-1)\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Flatten: 28x28 -> 784\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# 3. Build the MLP Model\n",
        "model = models.Sequential([\n",
        "    layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')])\n",
        "\n",
        "# Show model summary\n",
        "model.summary()\n",
        "\n",
        "# 4. Compile the Model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 5. Train the Model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=128,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6. Evaluate the Model on Test Data\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"\\nTest Accuracy:\", test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "id": "f2kc0V1bXhq6",
        "outputId": "13e78f5f-5857-4654-cd62-2248d6d93b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m100,480\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">100,480</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,386\u001b[0m (427.29 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,386</span> (427.29 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8069 - loss: 0.6648 - val_accuracy: 0.9588 - val_loss: 0.1505\n",
            "Epoch 2/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9519 - loss: 0.1606 - val_accuracy: 0.9650 - val_loss: 0.1184\n",
            "Epoch 3/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9683 - loss: 0.1060 - val_accuracy: 0.9727 - val_loss: 0.0950\n",
            "Epoch 4/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9766 - loss: 0.0782 - val_accuracy: 0.9760 - val_loss: 0.0785\n",
            "Epoch 5/5\n",
            "\u001b[1m422/422\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9832 - loss: 0.0579 - val_accuracy: 0.9763 - val_loss: 0.0794\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 0.0996\n",
            "\n",
            "Test Accuracy: 0.9735999703407288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rcDM7ZyWmcwm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why does TensorFlow produce higher accuracy than sklearn’s MLPClassifier even when both use similar architectures and parameters?\n",
        "2. Why does TensorFlow run significantly faster per epoch even with the same batch size and architecture?\n",
        "3. How does one-hot encoding in TensorFlow influence the learning dynamics compared to sklearn’s direct integer label training?\n",
        "4. Why does TensorFlow require only pixel normalization (/255), while sklearn MLP requires strong normalization (StandardScaler)? What happens mathematically to the weight updates if scaling is not done?\n",
        "5. How does GPU acceleration in TensorFlow vs pure CPU computation in sklearn change the weight update patterns, convergence speed, and final accuracy?"
      ],
      "metadata": {
        "id": "SYc49mDomeWd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy Differences\n",
        "Even with similar architectures (128 and 64 hidden units), TensorFlow often achieves higher accuracy due to:\n",
        "\n",
        "\n",
        "\n",
        "Weight Initialization: TensorFlow (Keras) and Scikit-learn use different default schemes for initializing weights, which significantly impacts how quickly and effectively a model finds the global minimum.\n",
        "\n",
        "\n",
        "Hyperparameter Defaults: Subtitle differences in default parameters for the 'adam' solver, such as epsilon or learning rate schedules, can lead to better fine-tuning in TensorFlow.\n",
        "\n",
        "\n",
        "2. Execution Speed per Epoch\n",
        "TensorFlow runs significantly faster per epoch  because:\n",
        "\n",
        "\n",
        "Library Optimization: TensorFlow is built on a highly optimized C++ backend and uses Eigen or MKL libraries specifically designed for high-performance tensor operations.\n",
        "\n",
        "\n",
        "Vectorization: TensorFlow handles large-scale matrix multiplications more efficiently than Scikit-learn’s implementation, which is primarily CPU-bound and not designed for deep learning at scale.\n",
        "\n",
        "\n",
        "3. One-Hot Encoding vs. Integer Labels\n",
        "\n",
        "TensorFlow: Uses one-hot encoding paired with categorical_crossentropy. This mathematically treats the output as a probability distribution across 10 distinct classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Scikit-learn: MLPClassifier handles integer labels directly. While functionally similar for the user, one-hot encoding in TensorFlow allows for explicit control over the loss calculation per class, which can lead to more stable gradient updates in multi-class environments.\n",
        "\n",
        "4. Normalization Requirements\n",
        "Scikit-learn: Requires StandardScaler because its solvers are highly sensitive to the scale of the input features. Without it, features with larger ranges would dominate the weight updates.\n",
        "\n",
        "\n",
        "TensorFlow: Often converges with simple pixel normalization (/255.0) 14because modern deep learning optimizers and weight initializers are more robust to input scales15.\n",
        "\n",
        "Mathematical Impact: If scaling is not performed, gradients can become very large (exploding) or very small (vanishing). This causes weight updates to oscillate wildly or stop entirely, preventing the model from converging to a solution.\n",
        "\n",
        "\n",
        "5. GPU Acceleration vs. CPU Computation\n",
        "\n",
        "Convergence Speed: GPU acceleration in TensorFlow allows for massive parallelization of weight updates, leading to much faster convergence in terms of clock time.\n",
        "\n",
        "\n",
        "Weight Update Patterns: While the mathematical formula for a weight update is the same, GPUs allow for larger batch processing and more complex architectures to be trained in the same timeframe it takes a CPU to run a single epoch.\n",
        "\n",
        "\n",
        "Final Accuracy: While a CPU can reach the same accuracy given enough time, the speed of GPUs allows for more iterations (epochs) and hyperparameter tuning, which ultimately results in higher final accuracy in practice."
      ],
      "metadata": {
        "id": "jjxrnmsGmf_u"
      }
    }
  ]
}