{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNg+QgGa9FG6G4WwtctE0uJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raka7317/Adventure_project-web-d-/blob/main/lab_2(n_gram).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7NJkK1NQKO-",
        "outputId": "50f7a575-be7f-4394-ef5d-2cbbdb25a873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['data', 'science', 'is', 'fun', 'and', 'data', 'science', 'is', 'powerful']\n",
            "\n",
            "BIGRAM PERPLEXITY\n",
            "Without smoothing: 1.189207115002721\n",
            "With Laplace smoothing: 3.158758147025058\n",
            "\n",
            "TRIGRAM PERPLEXITY\n",
            "Without smoothing: 1.2190136542044754\n",
            "With Laplace smoothing: 3.364298418765503\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# 1. TEXT PREPROCESSING\n",
        "# -----------------------------\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2. TOKENIZATION\n",
        "# -----------------------------\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3. GENERATE N-GRAMS\n",
        "# -----------------------------\n",
        "def generate_ngrams(tokens, n):\n",
        "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4. PROBABILITY WITHOUT SMOOTHING\n",
        "# -----------------------------\n",
        "def ngram_prob_no_smoothing(ngrams, lower_ngrams):\n",
        "    ngram_count = Counter(ngrams)\n",
        "    lower_count = Counter(lower_ngrams)\n",
        "\n",
        "    probs = {}\n",
        "    for ngram in ngram_count:\n",
        "        prefix = ngram[:-1]\n",
        "        probs[ngram] = ngram_count[ngram] / lower_count[prefix]\n",
        "    return probs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 5. PROBABILITY WITH LAPLACE SMOOTHING\n",
        "# -----------------------------\n",
        "def ngram_prob_laplace(ngrams, lower_ngrams, vocab_size):\n",
        "    ngram_count = Counter(ngrams)\n",
        "    lower_count = Counter(lower_ngrams)\n",
        "\n",
        "    probs = {}\n",
        "    for ngram in ngram_count:\n",
        "        prefix = ngram[:-1]\n",
        "        probs[ngram] = (ngram_count[ngram] + 1) / (lower_count[prefix] + vocab_size)\n",
        "    return probs\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6. PERPLEXITY CALCULATION\n",
        "# -----------------------------\n",
        "def calculate_perplexity(ngrams, probabilities):\n",
        "    N = len(ngrams)\n",
        "    log_sum = 0\n",
        "\n",
        "    for ngram in ngrams:\n",
        "        prob = probabilities.get(ngram, 1e-10)  # avoid log(0)\n",
        "        log_sum += math.log(prob)\n",
        "\n",
        "    return math.exp(-log_sum / N)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7. MAIN EXECUTION\n",
        "# -----------------------------\n",
        "text = \"Data science is fun and data science is powerful\"\n",
        "\n",
        "# Preprocessing\n",
        "clean_text = preprocess_text(text)\n",
        "\n",
        "# Tokenization\n",
        "tokens = tokenize(clean_text)\n",
        "vocab_size = len(set(tokens))\n",
        "\n",
        "# -------- BIGRAM --------\n",
        "bigrams = generate_ngrams(tokens, 2)\n",
        "unigrams = [(token,) for token in tokens]\n",
        "\n",
        "bigram_probs_no = ngram_prob_no_smoothing(bigrams, unigrams)\n",
        "bigram_pp_no = calculate_perplexity(bigrams, bigram_probs_no)\n",
        "\n",
        "bigram_probs_la = ngram_prob_laplace(bigrams, unigrams, vocab_size)\n",
        "bigram_pp_la = calculate_perplexity(bigrams, bigram_probs_la)\n",
        "\n",
        "# -------- TRIGRAM --------\n",
        "trigrams = generate_ngrams(tokens, 3)\n",
        "bigram_prefixes = generate_ngrams(tokens, 2)\n",
        "\n",
        "trigram_probs_no = ngram_prob_no_smoothing(trigrams, bigram_prefixes)\n",
        "trigram_pp_no = calculate_perplexity(trigrams, trigram_probs_no)\n",
        "\n",
        "trigram_probs_la = ngram_prob_laplace(trigrams, bigram_prefixes, vocab_size)\n",
        "trigram_pp_la = calculate_perplexity(trigrams, trigram_probs_la)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 8. OUTPUT\n",
        "# -----------------------------\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "print(\"\\nBIGRAM PERPLEXITY\")\n",
        "print(\"Without smoothing:\", bigram_pp_no)\n",
        "print(\"With Laplace smoothing:\", bigram_pp_la)\n",
        "\n",
        "print(\"\\nTRIGRAM PERPLEXITY\")\n",
        "print(\"Without smoothing:\", trigram_pp_no)\n",
        "print(\"With Laplace smoothing:\", trigram_pp_la)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "1Hqdx8R_RM8L"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus={ \"<s> NLP is fun </s>\",\n",
        "        \"<s> NLP is powrful </s>\"}\n",
        "\n",
        "unigram=defaultdict(int)\n",
        "\n",
        "for sentence in corpus:\n",
        "    words=sentence.split()\n",
        "    for word in words:\n",
        "        unigram[word]+=1\n",
        "\n",
        "print(\"unigram couts:\")\n",
        "for word, count in unigram.items():\n",
        "  print(word, \":\", count)\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n vacabulary size : \",len(unigram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDCc3Vh8RQda",
        "outputId": "a6d37590-99b4-401c-ba1e-b9ce910bde89"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unigram couts:\n",
            "<s> : 2\n",
            "NLP : 2\n",
            "is : 2\n",
            "fun : 1\n",
            "</s> : 2\n",
            "powrful : 1\n",
            "\n",
            " vacabulary size :  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "corpus={ \"<s>i love NLP </s>\",\n",
        "        \"<s> i love ML </s>\"}\n",
        "\n",
        "bigram=defaultdict(int)\n",
        "\n",
        "for sentence in corpus:\n",
        "    words=sentence.split()\n",
        "    for i in range(len(words)-1):\n",
        "        bg=(words[i],words[i+1])\n",
        "        bigram[bg]+=1\n",
        "\n",
        "print(\"birgram counts:\")\n",
        "for bg, count in bigram.items():\n",
        "  print(bg, \":\", count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n vacabulary size : \",len(bigram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8n-qt3IVxR7",
        "outputId": "6f3b1a80-3755-440b-d7a7-918250b8736f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "birgram counts:\n",
            "('<s>i', 'love') : 1\n",
            "('love', 'NLP') : 1\n",
            "('NLP', '</s>') : 1\n",
            "('<s>', 'i') : 1\n",
            "('i', 'love') : 1\n",
            "('love', 'ML') : 1\n",
            "('ML', '</s>') : 1\n",
            "\n",
            " vacabulary size :  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "corpus={ \"<s> data science is fun </s>\",\n",
        "        \"<s>  data science is useful</s>\"}\n",
        "\n",
        "trigram=defaultdict(int)\n",
        "\n",
        "for sentence in corpus:\n",
        "    words=sentence.split()\n",
        "    for i in range(len(words)-2):\n",
        "        tg=(words[i],words[i+1],words[i+2])\n",
        "        trigram[tg]+=1\n",
        "\n",
        "print(\"trigram  counts:\")\n",
        "for tg, count in bigram.items():\n",
        "  print(tg, \":\", count)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n vacabulary size : \",len(bigram))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqKJOj63V3wT",
        "outputId": "665c8603-0506-49dc-8586-6c80a024c25f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trigram  counts:\n",
            "('<s>i', 'love') : 1\n",
            "('love', 'NLP') : 1\n",
            "('NLP', '</s>') : 1\n",
            "('<s>', 'i') : 1\n",
            "('i', 'love') : 1\n",
            "('love', 'ML') : 1\n",
            "('ML', '</s>') : 1\n",
            "\n",
            " vacabulary size :  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  collection import defaultdict\n",
        "import math\n",
        "print(\" enter training sentences (use<s> and </s>)\")\n",
        "print(\" enter one sentence per line\")\n",
        "print(\" type end to finish \\n\")\n",
        "corpus=[]\n",
        "while True:\n",
        "   line =input()\n",
        "   if line.strip().upper()==END:\n",
        "      break\n",
        "   corpus.append(line.strip().split)\n",
        "\n",
        "print(\"\\n training corpus: \")\n",
        "for s in corpus:\n",
        "  print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "Bkb5rL2bYIOD",
        "outputId": "1bc68cd0-1e93-4bb9-e605-41c6fc6ac5d2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'collection'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2386028287.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mcollection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" enter training sentences (use<s> and </s>)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" enter one sentence per line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" type end to finish \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'collection'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axIB1JL9ZvgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UQ2xjVJ1ZwcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# FIX 1: Define the stop variable (sentinel) used in the loop\n",
        "END_SENTINEL = \"END\"\n",
        "\n",
        "print(\"Enter training sentences (use <s> and </s> markers for start/end).\")\n",
        "print(\"Enter one sentence per line.\")\n",
        "print(f\"Type '{END_SENTINEL}' to finish input. \\n\")\n",
        "\n",
        "corpus = []\n",
        "while True:\n",
        "    line = input()\n",
        "\n",
        "    # 4 SPACES: Check the stop condition\n",
        "    # FIX 2: Use the defined END_SENTINEL instead of END\n",
        "    if line.strip().upper() == END_SENTINEL:\n",
        "        break\n",
        "\n",
        "    # 4 SPACES: This line is correctly inside the while loop now\n",
        "    # FIX 3: Add parentheses to .split() to make it a function call\n",
        "    if line.strip(): # Only process non-empty lines\n",
        "        corpus.append(line.strip().split())\n",
        "\n",
        "print(\"\\n--- Training Corpus (Tokenized) ---\")\n",
        "for s in corpus:\n",
        "    print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "sQp9doknZmsD",
        "outputId": "a029a200-0c45-4a81-d33d-ed5844b8e41e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter training sentences (use <s> and </s> markers for start/end).\n",
            "Enter one sentence per line.\n",
            "Type 'END' to finish input. \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4179470795.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# 4 SPACES: Check the stop condition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  collection import defaultdict\n",
        "import math\n",
        "print(\" enter training sentences (use<s> and </s>)\")\n",
        "print(\" enter one sentence per line\")\n",
        "print(\" type end to finish \\n\")\n",
        "corpus=[]\n",
        "while True:\n",
        "   line =input()\n",
        "   if line.strip().upper()==END:\n",
        "      break\n",
        "   corpus.append(line.strip().split)\n",
        "\n",
        "print(\"\\n training corpus: \")\n",
        "for s in corpus:\n",
        "  print(s)\n",
        "\n",
        "\n",
        "test_sentence=input(\"\\n enter test sentence: \")\n",
        "test_tokens=test_sentence.split()\n",
        "\n",
        "\n",
        "unigram=defaultdict(int)\n",
        "bigram=defaultdict(int)\n",
        "trigram=defaultdict(int)\n",
        "\n",
        "vocab=set()\n",
        "\n",
        "for sent in corpus:\n",
        "   for i in range(len(sent)):\n",
        "     unigram[sent[i]]+=1\n",
        "     vocab.add(sent[i])\n",
        "\n",
        "     if i<len(sent)-1:\n",
        "       bigram[(sent[i], sent[i+1])]+=1\n",
        "\n",
        "     if i<len(sent)-1:\n",
        "       bigram[(sent[i], sent[i+1],sent[i+2])]+=1\n",
        "\n",
        "V=len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "OYPf1U6bZx-h",
        "outputId": "d3a9a90b-1522-4fa9-8333-d0a2b3cbd687"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'defaultdict' from 'collection' (/usr/local/lib/python3.12/dist-packages/collection/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-627648531.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mcollection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" enter training sentences (use<s> and </s>)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" enter one sentence per line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" type end to finish \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'defaultdict' from 'collection' (/usr/local/lib/python3.12/dist-packages/collection/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install collection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTITBULxbLNp",
        "outputId": "8a4404dc-bd14-40e7-c078-5a0cd7fc9ae5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting collection\n",
            "  Downloading collection-0.1.6.tar.gz (5.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: collection\n",
            "  Building wheel for collection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for collection: filename=collection-0.1.6-py3-none-any.whl size=5098 sha256=814bedff156a193cf9ef032804c071a73af73cdb1d06f373c9cb6f85f38e8d8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/bb/2e/e430efbb8e7a845a40292527c78c51d201db424b763ae2ccdb\n",
            "Successfully built collection\n",
            "Installing collected packages: collection\n",
            "Successfully installed collection-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Define the stop word for dynamic input\n",
        "END_SENTINEL = \"END\"\n",
        "\n",
        "print(\"Enter training sentences (use <s> and </s> markers).\")\n",
        "print(\"Enter one sentence per line.\")\n",
        "print(f\"Type '{END_SENTINEL}' to finish input. \\n\")\n",
        "\n",
        "corpus = []\n",
        "while True:\n",
        "    line = input()\n",
        "\n",
        "    # 1. Check for the stop condition\n",
        "    if line.strip().upper() == END_SENTINEL:\n",
        "        break\n",
        "\n",
        "    # 2. Tokenize and append (FIX: added parentheses to .split())\n",
        "    if line.strip():\n",
        "        corpus.append(line.strip().split())\n",
        "\n",
        "print(\"\\n--- Training Corpus (Tokenized) ---\")\n",
        "for s in corpus:\n",
        "    print(s)\n",
        "\n",
        "# Get test sentence input\n",
        "test_sentence_raw = input(\"\\n Enter test sentence: \")\n",
        "test_tokens = test_sentence_raw.strip().split()\n",
        "\n",
        "\n",
        "# --- N-GRAM COUNTING ---\n",
        "unigram = defaultdict(int)\n",
        "bigram = defaultdict(int)\n",
        "trigram = defaultdict(int)\n",
        "vocab = set()\n",
        "\n",
        "for sent in corpus:\n",
        "    # We iterate up to len(sent)\n",
        "    for i in range(len(sent)):\n",
        "\n",
        "        # 1. Unigram Count and Vocabulary\n",
        "        unigram[sent[i]] += 1\n",
        "        vocab.add(sent[i])\n",
        "\n",
        "        # 2. Bigram Count (W_i, W_i+1)\n",
        "        # We need at least one more word, so stop one token early\n",
        "        if i < len(sent) - 1:\n",
        "            bigram[(sent[i], sent[i+1])] += 1\n",
        "\n",
        "        # 3. Trigram Count (W_i, W_i+1, W_i+2)\n",
        "        # We need at least two more words, so stop two tokens early\n",
        "        if i < len(sent) - 2:\n",
        "            # FIX: Used 'trigram' dictionary instead of 'bigram'\n",
        "            trigram[(sent[i], sent[i+1], sent[i+2])] += 1\n",
        "\n",
        "V = len(vocab)\n",
        "\n",
        "# --- Output Results ---\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "print(f\"| {'N-GRAM COUNTING RESULTS':^38} |\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"| {'Vocabulary Size (V)':<25} | {V:>10} |\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "print(\"\\n### Unigram Counts ###\")\n",
        "print(unigram)\n",
        "\n",
        "print(\"\\n### Bigram Counts ###\")\n",
        "print(bigram)\n",
        "\n",
        "print(\"\\n### Trigram Counts ###\")\n",
        "\n",
        "print(trigram)\n",
        "\n",
        "\n",
        "\n",
        "# laplace smoothing\n",
        "\n",
        "def bigram_log_prob(sentence):\n",
        "  logp=0.0\n",
        "  for i in range(len(sentence)-1):\n",
        "   w1,w2=sentence[i],sentence[i+1]\n",
        "   prob=(bigram[(w1,w2)]+1)/(unigram[w1]+V)\n",
        "  return logp\n",
        "\n",
        "\n",
        "def trigram_log_prob(sentence):\n",
        "  logp: 0.0\n",
        "  for i in range(len(sentence)-2):\n",
        "    w1,w2,w3=sentence[i],sentence[i+1],sentence[i+2]\n",
        "    prob=(trigram[(w1,w2,w3)]+1)/(bigram[(w1,w2)]+V)\n",
        "    logp+=math.log(prob)\n",
        "  return logp\n",
        "\n",
        "\n",
        "\n",
        "  def perplexity(logp,sentence):\n",
        "    N=len(sentence)-1\n",
        "    return math.exp(-logp/N)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #computer perplexity\n",
        "\n",
        "\n",
        "  pp_bigram=perplexity(bigram_log_prob(test_sentence),test_sentence)\n",
        "  pp_trigram=perplexity(trigram_log_prob(test_sentence),test_sentence)\n",
        "\n",
        "\n",
        "\n",
        "  #output\n",
        "\n",
        "  print(\"\\n result\\n\")\n",
        "  print(\"vacabulary size |v| = \",V)\n",
        "  print(\"bigram perplexity : \", round(pp_bigram,4))\n",
        "  print(\"trigram perplexity : \", round(pp_trigram,4))\n",
        "\n",
        "\n",
        "\n",
        "  print(\"\\n model comparison \\n\")\n",
        "\n",
        "  if pp_trigram<pp_bigram:\n",
        "      print(\"trigram model is better\")\n",
        "  else:\n",
        "    print(\"bigram model is better.... \")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw1BNgP2bNoa",
        "outputId": "1f897596-dd60-4fe1-fdd2-96b6f5aaefd3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter training sentences (use <s> and </s> markers).\n",
            "Enter one sentence per line.\n",
            "Type 'END' to finish input. \n",
            "\n",
            "<s> my name is rakesh</s>\n",
            "<s>rakesh is good </s>\n",
            "END\n",
            "\n",
            "--- Training Corpus (Tokenized) ---\n",
            "['<s>', 'my', 'name', 'is', 'rakesh</s>']\n",
            "['<s>rakesh', 'is', 'good', '</s>']\n",
            "\n",
            " Enter test sentence: rakesh is master\n",
            "\n",
            "========================================\n",
            "|        N-GRAM COUNTING RESULTS         |\n",
            "========================================\n",
            "| Vocabulary Size (V)       |          8 |\n",
            "----------------------------------------\n",
            "\n",
            "### Unigram Counts ###\n",
            "defaultdict(<class 'int'>, {'<s>': 1, 'my': 1, 'name': 1, 'is': 2, 'rakesh</s>': 1, '<s>rakesh': 1, 'good': 1, '</s>': 1})\n",
            "\n",
            "### Bigram Counts ###\n",
            "defaultdict(<class 'int'>, {('<s>', 'my'): 1, ('my', 'name'): 1, ('name', 'is'): 1, ('is', 'rakesh</s>'): 1, ('<s>rakesh', 'is'): 1, ('is', 'good'): 1, ('good', '</s>'): 1})\n",
            "\n",
            "### Trigram Counts ###\n",
            "defaultdict(<class 'int'>, {('<s>', 'my', 'name'): 1, ('my', 'name', 'is'): 1, ('name', 'is', 'rakesh</s>'): 1, ('<s>rakesh', 'is', 'good'): 1, ('is', 'good', '</s>'): 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " from collections import defaultdict\n",
        "import math\n",
        "from itertools import chain\n",
        "\n",
        "# Define the stop word for dynamic input\n",
        "END_SENTINEL = \"END\"\n",
        "SMOOTHING_K = 1 # k=1 for Add-One (Laplace) Smoothing\n",
        "\n",
        "print(\"Enter training sentences (use <s> and </s> markers).\")\n",
        "print(\"Enter one sentence per line.\")\n",
        "print(f\"Type '{END_SENTINEL}' to finish input. \\n\")\n",
        "\n",
        "corpus = []\n",
        "while True:\n",
        "    line = input()\n",
        "\n",
        "    if line.strip().upper() == END_SENTINEL:\n",
        "        break\n",
        "\n",
        "    if line.strip():\n",
        "        corpus.append(line.strip().split())\n",
        "\n",
        "print(\"\\n--- Training Corpus (Tokenized) ---\")\n",
        "for s in corpus:\n",
        "    print(s)\n",
        "\n",
        "# Get test sentence input and tokenize (ensure to add start/end markers later)\n",
        "test_sentence_raw = input(\"\\n Enter test sentence: \")\n",
        "test_tokens_raw = test_sentence_raw.strip().split()\n",
        "\n",
        "\n",
        "# --- N-GRAM COUNTING (Training Data) ---\n",
        "unigram = defaultdict(int)\n",
        "bigram = defaultdict(int)\n",
        "trigram = defaultdict(int)\n",
        "vocab = set()\n",
        "\n",
        "for sent in corpus:\n",
        "    for i in range(len(sent)):\n",
        "\n",
        "        # 1. Unigram Count and Vocabulary\n",
        "        unigram[sent[i]] += 1\n",
        "        vocab.add(sent[i])\n",
        "\n",
        "        # 2. Bigram Count\n",
        "        if i < len(sent) - 1:\n",
        "            bigram[(sent[i], sent[i+1])] += 1\n",
        "\n",
        "        # 3. Trigram Count\n",
        "        if i < len(sent) - 2:\n",
        "            trigram[(sent[i], sent[i+1], sent[i+2])] += 1\n",
        "\n",
        "V = len(vocab)\n",
        "\n",
        "\n",
        "# --- PROBABILITY AND PERPLEXITY FUNCTIONS ---\n",
        "\n",
        "def bigram_log_prob(tokens_list, V, k=SMOOTHING_K):\n",
        "    \"\"\"Calculates the log probability of a sentence using smoothed Bigram model.\"\"\"\n",
        "\n",
        "    # Need to add <s> marker for the first Bigram context\n",
        "    sentence = ['<s>'] + tokens_list\n",
        "\n",
        "    # FIX: Initialize logp correctly\n",
        "    logp = 0.0\n",
        "\n",
        "    # We iterate from the first word W_1 (which follows <s>) up to the last word W_N\n",
        "    for i in range(len(sentence) - 1):\n",
        "        w1, w2 = sentence[i], sentence[i+1] # w1 is context, w2 is current word\n",
        "\n",
        "        # Get counts from training data (0 if unseen)\n",
        "        count_w1 = unigram.get(w1, 0)\n",
        "        count_bigram = bigram.get((w1, w2), 0)\n",
        "\n",
        "        # Laplace Smoothing: (Count(W1, W2) + k) / (Count(W1) + V*k)\n",
        "        prob = (count_bigram + k) / (count_w1 + V * k)\n",
        "\n",
        "        # FIX: Update logp inside the loop\n",
        "        # We use math.log (natural log) here, so perplexity must use math.exp\n",
        "        logp += math.log(prob)\n",
        "\n",
        "    return logp\n",
        "\n",
        "\n",
        "def trigram_log_prob(tokens_list, V, k=SMOOTHING_K):\n",
        "    \"\"\"Calculates the log probability of a sentence using smoothed Trigram model.\"\"\"\n",
        "\n",
        "    # Need to add <s> <s> markers for the first two Trigram contexts\n",
        "    sentence = ['<s>', '<s>'] + tokens_list\n",
        "\n",
        "    # FIX: Correctly initialize logp\n",
        "    logp = 0.0\n",
        "\n",
        "    # We iterate from the second word W_2 up to the last word W_N\n",
        "    for i in range(len(sentence) - 2):\n",
        "        w1, w2, w3 = sentence[i], sentence[i+1], sentence[i+2] # (w1, w2) is context, w3 is current word\n",
        "\n",
        "        # Get counts from training data (0 if unseen)\n",
        "        count_w1w2 = bigram.get((w1, w2), 0)\n",
        "        count_trigram = trigram.get((w1, w2, w3), 0)\n",
        "\n",
        "        # Laplace Smoothing: (Count(W1, W2, W3) + k) / (Count(W1, W2) + V*k)\n",
        "        prob = (count_trigram + k) / (count_w1w2 + V * k)\n",
        "\n",
        "        logp += math.log(prob)\n",
        "\n",
        "    return logp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# FIX: Corrected indentation for the function definition\n",
        "def perplexity(logp, N):\n",
        "    \"\"\"\n",
        "    Calculates Perplexity.\n",
        "    N is the number of predictions made (tokens - N_gram_order + 1, e.g., tokens - 1 for bigram).\n",
        "    \"\"\"\n",
        "    if N == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    # Formula: exp(-1/N * logP) because we used math.log (natural log)\n",
        "    #\n",
        "    return math.exp(-logp / N)\n",
        "\n",
        "\n",
        "# --- EXECUTION AND OUTPUT ---\n",
        "\n",
        "# Determine the number of predictions (N) for the test sentence\n",
        "# For a bigram model, we make N_tokens predictions (W_1, W_2, ..., W_N)\n",
        "N_bigram = len(test_tokens_raw)\n",
        "N_trigram = len(test_tokens_raw)\n",
        "\n",
        "# Calculate log probabilities\n",
        "lp_bigram = bigram_log_prob(test_tokens_raw, V)\n",
        "lp_trigram = trigram_log_prob(test_tokens_raw, V)\n",
        "\n",
        "# Calculate perplexity\n",
        "pp_bigram = perplexity(lp_bigram, N_bigram)\n",
        "pp_trigram = perplexity(lp_trigram, N_trigram)\n",
        "\n",
        "\n",
        "# --- Output Results (FIX: Corrected indentation for final output) ---\n",
        "\n",
        "# Output Counts (kept for debugging)\n",
        "print(\"\\n\" + \"=\" * 40)\n",
        "print(f\"| {'N-GRAM COUNTING RESULTS':^38} |\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"| {'Vocabulary Size (V)':<25} | {V:>10} |\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Test Sentence: '{test_sentence_raw}'\")\n",
        "\n",
        "print(\"\\n### Unigram Counts ###\")\n",
        "print(unigram)\n",
        "print(\"\\n### Bigram Counts ###\")\n",
        "print(bigram)\n",
        "print(\"\\n### Trigram Counts ###\")\n",
        "print(trigram)\n",
        "\n",
        "print(\"\\n\\n--- Final Results ---\\n\")\n",
        "print(f\"Vocabulary Size |V| = {V}\")\n",
        "print(f\"Bigram Perplexity : {pp_bigram:.4f}\")\n",
        "print(f\"Trigram Perplexity : {pp_trigram:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Model Comparison ---\")\n",
        "if pp_trigram < pp_bigram:\n",
        "    print(\"Trigram model is better (lower perplexity).\")\n",
        "else:\n",
        "    print(\"Bigram model is better (lower perplexity) or results are close.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzKyfPHQc0Dj",
        "outputId": "676ee410-2699-4fca-9676-dc5be933f912"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter training sentences (use <s> and </s> markers).\n",
            "Enter one sentence per line.\n",
            "Type 'END' to finish input. \n",
            "\n",
            "<s> i love data science</s>\n",
            "<s> data science is good </s>\n",
            "END\n",
            "\n",
            "--- Training Corpus (Tokenized) ---\n",
            "['<s>', 'i', 'love', 'data', 'science</s>']\n",
            "['<s>', 'data', 'science', 'is', 'good', '</s>']\n",
            "\n",
            " Enter test sentence: data science is field\n",
            "\n",
            "========================================\n",
            "|        N-GRAM COUNTING RESULTS         |\n",
            "========================================\n",
            "| Vocabulary Size (V)       |          9 |\n",
            "----------------------------------------\n",
            "Test Sentence: 'data science is field'\n",
            "\n",
            "### Unigram Counts ###\n",
            "defaultdict(<class 'int'>, {'<s>': 2, 'i': 1, 'love': 1, 'data': 2, 'science</s>': 1, 'science': 1, 'is': 1, 'good': 1, '</s>': 1})\n",
            "\n",
            "### Bigram Counts ###\n",
            "defaultdict(<class 'int'>, {('<s>', 'i'): 1, ('i', 'love'): 1, ('love', 'data'): 1, ('data', 'science</s>'): 1, ('<s>', 'data'): 1, ('data', 'science'): 1, ('science', 'is'): 1, ('is', 'good'): 1, ('good', '</s>'): 1})\n",
            "\n",
            "### Trigram Counts ###\n",
            "defaultdict(<class 'int'>, {('<s>', 'i', 'love'): 1, ('i', 'love', 'data'): 1, ('love', 'data', 'science</s>'): 1, ('<s>', 'data', 'science'): 1, ('data', 'science', 'is'): 1, ('science', 'is', 'good'): 1, ('is', 'good', '</s>'): 1})\n",
            "\n",
            "\n",
            "--- Final Results ---\n",
            "\n",
            "Vocabulary Size |V| = 9\n",
            "Bigram Perplexity : 6.2363\n",
            "Trigram Perplexity : 6.8872\n",
            "\n",
            "--- Model Comparison ---\n",
            "Bigram model is better (lower perplexity) or results are close.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigram, bigram, trigram, without smoothing , with smoothing , find perplexity,\n",
        "training corpus, find valcbulary size,\n",
        "and also word count of all n_grams"
      ],
      "metadata": {
        "id": "evtqHn6gliy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}